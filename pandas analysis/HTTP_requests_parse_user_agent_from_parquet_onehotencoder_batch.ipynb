{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "DATA_DIR = '/home/nsuprotivniy/Documents/Работа/OKru/Antispam/data/'\n",
    "GRAPH_PATH = '/home/nsuprotivniy/Documents/Работа/OKru/Antispam/graph/'\n",
    "MODEL_DIR = '/home/nsuprotivniy/Documents/Работа/OKru/Antispam/model/'\n",
    "SAMPLE_FRAC = 0.01 \n",
    "BATCH_SIZE = 1_000_000\n",
    "\n",
    "SVM_ALPHA_RANGE = np.fromfunction(lambda i: 1e-15 * pow(10, i), (15,), dtype=float)\n",
    "SVM_MAX_ITER_RANGE = np.fromfunction(lambda i: 10 * (i + 1), (20,), dtype=float)\n",
    "\n",
    "FEATURES = [\n",
    "    'DeviceClass',\n",
    "    'DeviceName',\n",
    "    'DeviceBrand',\n",
    "    'DeviceCpu',\n",
    "    'DeviceCpuBits',\n",
    "    'OperatingSystemClass',\n",
    "    'OperatingSystemName',\n",
    "    'OperatingSystemVersion',\n",
    "    'OperatingSystemNameVersion',\n",
    "    'OperatingSystemVersionBuild',\n",
    "    'LayoutEngineClass',\n",
    "    'LayoutEngineName',\n",
    "    'LayoutEngineVersion',\n",
    "    'LayoutEngineVersionMajor',\n",
    "    'LayoutEngineNameVersion',\n",
    "    'LayoutEngineNameVersionMajor',\n",
    "    'AgentClass',\n",
    "    'AgentName',\n",
    "    'AgentVersion',\n",
    "    'AgentVersionMajor',\n",
    "    'AgentNameVersion',\n",
    "    'AgentNameVersionMajor',\n",
    "#     'from',\n",
    "#     'to',\n",
    "#     'url',\n",
    "#     'requestType',\n",
    "    'operation'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /home/nsuprotivniy/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six>=1.0.0 in /home/nsuprotivniy/anaconda3/lib/python3.6/site-packages (from pyarrow)\n",
      "Requirement already satisfied: numpy>=1.10 in /home/nsuprotivniy/anaconda3/lib/python3.6/site-packages (from pyarrow)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import seaborn as sns\n",
    "import user_agents\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготока моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing Trick - единственный нормальный подход https://habr.com/company/ods/blog/326418/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashingTrick(TransformerMixin):\n",
    "    \n",
    "    _default_hashing_trick_modulars = {\n",
    "        'DeviceClass': 10000,\n",
    "        'DeviceName': 10000,\n",
    "        'DeviceBrand': 10000,\n",
    "        'DeviceCpu': 10000,\n",
    "        'DeviceCpuBits': 10000,\n",
    "        'OperatingSystemClass': 10000,\n",
    "        'OperatingSystemName': 10000,\n",
    "        'OperatingSystemVersion': 10000,\n",
    "        'OperatingSystemNameVersion': 10000,\n",
    "        'OperatingSystemVersionBuild': 10000,\n",
    "        'LayoutEngineClass': 10000,\n",
    "        'LayoutEngineName': 10000,\n",
    "        'LayoutEngineVersion': 10000,\n",
    "        'LayoutEngineVersionMajor': 10000,\n",
    "        'LayoutEngineNameVersion': 10000,\n",
    "        'LayoutEngineNameVersionMajor': 10000,\n",
    "        'AgentClass': 10000,\n",
    "        'AgentName': 10000,\n",
    "        'AgentVersion': 10000,\n",
    "        'AgentVersionMajor': 10000,\n",
    "        'AgentNameVersion': 10000,\n",
    "        'AgentNameVersionMajor': 10000,\n",
    "        'from': 10000,\n",
    "        'to': 10000,\n",
    "        'url': 10000000,\n",
    "        'requestType': 10000,\n",
    "        'operation': 10000\n",
    "    }\n",
    "    \n",
    "    def __init__(self, hashing_trick_modulars = _default_hashing_trick_modulars):\n",
    "        self.hashing_trick_modulars = hashing_trick_modulars\n",
    "    \n",
    "    def set_params(self, **kwargs):\n",
    "        \"\"\"Set the parameters of this estimator.\"\"\"\n",
    "    \n",
    "    def get_params(self, **kwargs):\n",
    "        return {\"hashing_trick_modulars\": self.hashing_trick_modulars}\n",
    "        \n",
    "    def _hashing_trick(self, x, n):\n",
    "        return hash(x) % n\n",
    "\n",
    "    def _column_hashing_trick(self, col_name):\n",
    "        self.http[col_name] = self.http[col_name].apply(self._hashing_trick, args=(self.hashing_trick_modulars[col_name],))\n",
    "  \n",
    "    def fit_transform(self, X, *_):\n",
    "        return self.transform(X)\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        self.http = X\n",
    "        for feature in FEATURES:\n",
    "            self._column_hashing_trick(feature)\n",
    "\n",
    "        return self.http\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = HashingTrick()\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "clf = SGDClassifier(random_state=17, max_iter=1000, alpha=0.0000001, loss='log')\n",
    "pipeline = Pipeline(\n",
    "    [('ht', ht)] +\n",
    "    [('ohe', ohe)]+\n",
    "    [('sgd', clf)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск гиперпаратметров\n",
    "#### Используется RandomizedSearchCV, так полный GridSearchCV работает долго"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = pq.read_table(DATA_DIR + 'HTTPRequests-20180217_1718_parsedUAA.parquet', columns=FEATURES ).to_pandas().sample(frac=SAMPLE_FRAC)\n",
    "target = pq.read_table(DATA_DIR + 'HTTPRequests-20180217_1718_parsedUAA.parquet', columns=['isBot'] ).to_pandas().sample(frac=SAMPLE_FRAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_holdout, y_train, y_holdout = train_test_split(http, target, test_size=0.1, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   51.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sgd__max_iter': 50.0, 'sgd__alpha': 1e-15} 0.006613024458919903\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      users       0.99      1.00      1.00      1002\n",
      "       bots       0.00      0.00      0.00         9\n",
      "\n",
      "avg / total       0.98      0.99      0.99      1011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {'sgd__alpha': SVM_ALPHA_RANGE,\n",
    "              'sgd__max_iter': SVM_MAX_ITER_RANGE}\n",
    "search = RandomizedSearchCV(pipeline, params, cv=5, n_jobs=-1, verbose=True, scoring='f1', n_iter = 10)\n",
    "search.fit(X_train, y_train)\n",
    "print(search.best_params_, search.best_score_)\n",
    "print(classification_report(y_holdout, search.predict(X_holdout), target_names = ['users', 'bots']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание моделей по полученным из search гиперпараметрам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(random_state = 17, \n",
    "                    max_iter = search.best_params_['sgd__max_iter'], \n",
    "                    alpha = search.best_params_['sgd__alpha'],\n",
    "                    loss = 'log')\n",
    "ht = HashingTrick().fit(http)\n",
    "ohe = OneHotEncoder(handle_unknown='ignore').fit(ht.transform(http))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/nsuprotivniy/Documents/Работа/OKru/Antispam/model/http_ua_from_parquet_onehotencoder_sgd.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(ht, MODEL_DIR + 'http_ua_from_parquet_onehotencoder_hasging_trick.pkl') \n",
    "joblib.dump(ohe, MODEL_DIR + 'http_ua_from_parquet_onehotencoder_ohe.pkl')\n",
    "joblib.dump(clf, MODEL_DIR + 'http_ua_from_parquet_onehotencoder_sgd.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение на большой выборке "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = joblib.load(MODEL_DIR + 'http_ua_from_parquet_onehotencoder_hasging_trick.pkl') \n",
    "ohe = joblib.load(MODEL_DIR + 'http_ua_from_parquet_onehotencoder_ohe.pkl')\n",
    "clf = joblib.load(MODEL_DIR + 'http_ua_from_parquet_onehotencoder_sgd.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = pq.read_table(DATA_DIR + 'HTTPRequests-20180217_1718_parsedUAA.parquet', \n",
    "                        columns=FEATURES + ['isBot']).to_batches(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = True\n",
    "for batch in train_batches:\n",
    "    http = batch.to_pandas()\n",
    "    X = http[FEATURES]\n",
    "    y = http['isBot']\n",
    "    X = ht.transform(X)\n",
    "    X = ohe.transform(X)\n",
    "    if (first):\n",
    "        clf.fit(X, y) # Проблема, когда 'y' состоит только из пользователей\n",
    "        first = False\n",
    "    else:\n",
    "        clf.partial_fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/nsuprotivniy/Documents/Работа/OKru/Antispam/model/http_ua_from_parquet_onehotencoder_sgd_fitted.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, MODEL_DIR + 'http_ua_from_parquet_onehotencoder_sgd_fitted.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация для большой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = joblib.load(MODEL_DIR + 'http_ua_from_parquet_onehotencoder_hasging_trick.pkl') \n",
    "ohe = joblib.load(MODEL_DIR + 'http_ua_from_parquet_onehotencoder_ohe.pkl')\n",
    "clf = joblib.load(MODEL_DIR + 'http_ua_from_parquet_onehotencoder_sgd_fitted.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batches  = pq.read_table(DATA_DIR + 'HTTPRequests-20180217_1718_parsedUAA.parquet', \n",
    "                        columns=FEATURES + ['isBot']).to_batches(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_out = 'HTTPRequests-20180217_1718_parsedUA_predicted.parquet'\n",
    "predicted_writer = pq.ParquetWriter(DATA_DIR + predict_out, \n",
    "                                    pa.Table.from_pandas(pd.DataFrame({'predict_proba': []}, dtype=np.float64)).schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in predict_batches:\n",
    "    http = batch.to_pandas()\n",
    "    X = http[FEATURES]\n",
    "    X = ht.transform(X)\n",
    "    X = ohe.transform(X)\n",
    "    predicted = clf.predict_proba(X)\n",
    "    table = pa.Table.from_pandas(pd.DataFrame({'predict_proba': predicted[:,1]}, dtype=np.float64))\n",
    "    predicted_writer.write_table(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование для большой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_proba = pq.read_table(DATA_DIR + 'HTTPRequests-20180217_1718_parsedUA_predicted.parquet',\n",
    "                                columns=['predict_proba']).to_pandas()\n",
    "\n",
    "real = pq.read_table(DATA_DIR + 'HTTPRequests-20180217_1718_parsedUAA.parquet',\n",
    "                                columns=['isBot']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = 0.5\n",
    "predicted = (predicted_proba.values.ravel() > bound).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      users       0.99      1.00      1.00   1000000\n",
      "       bots       0.89      0.47      0.61     10377\n",
      "\n",
      "avg / total       0.99      0.99      0.99   1010377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(real, predicted, target_names = ['users', 'bots']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.005370272680395536)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_proba.values.min(), predicted_proba.values.max(), predicted_proba.values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_userId = pq.read_table(DATA_DIR + 'HTTPRequests-20180217_1718_parsedUAA.parquet',\n",
    "                                columns=['isBot', 'userId']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_userId['predict_proba'] = predicted_proba['predict_proba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = real_userId.groupby('userId').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped['predict'] = (grouped['predict_proba'] > bound).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      users       1.00      1.00      1.00    678654\n",
      "       bots       0.30      0.90      0.45       266\n",
      "\n",
      "avg / total       1.00      1.00      1.00    678920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(grouped['isBot'], grouped['predict'], target_names = ['users', 'bots']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
